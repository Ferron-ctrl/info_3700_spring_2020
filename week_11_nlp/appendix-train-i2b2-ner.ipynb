{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This notebook will walk through how to train a spaCy NER model using the i2b2 2012 Clinical Event Extraction shared task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import glob\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import spacy\n",
    "\n",
    "from spacy.tokens import Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import src.i2b2_utils\n",
    "# import constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define paths\n",
    "If you would like get access to the data for the challenge so you can actually train the model yourself, fill out this data access form on the i2b2 website: https://portal.dbmi.hms.harvard.edu/. Once you're approved, you can download the XML files and change the following path variables in the notebook to read in the original data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = '/Users/alec/Data/i2b2_2012'\n",
    "TRAINDIR = os.path.join(DATADIR, '2012-07-15.original-annotation.release')\n",
    "TESTDIR = os.path.join(DATADIR, '2012-08-08.test-data.event-timex-groundtruth/xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have the files downloaded, or if the above path variables are not correct, this notebook will use the sample data below. This won't be nearly enough to actually train a model (that takes several thousand examples), but it will be enough to be able to run the script as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't have \n",
    "EXAMPLE_TRAIN_DATA = [\n",
    "    (' While this patient has had no complaints\\nof gastrointestinal bleed in the past,'\n",
    "     ' he should be sent for\\nendoscopy to look for varices as an outpatient.',\n",
    "     {'entities': [(45, 67, 'PROBLEM'),\n",
    "                   (103, 112, 'TEST'),\n",
    "                   (125, 132, 'PROBLEM')]})\n",
    "]\n",
    "\n",
    "EXAMPLE_TEST_DATA = [\n",
    "    ('The patient has allergy to Bactrim that causes\\nhim to have a rash.',\n",
    "     {'entities': [\n",
    "         (16, 23, 'PROBLEM'),\n",
    "           (27, 34, 'TREATMENT'),\n",
    "           (59, 66, 'PROBLEM')]})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i2b2 data found. Will use that to train.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(DATADIR):\n",
    "    print(\"i2b2 data found. Will use that to train.\")\n",
    "    USE_I2B2 = True\n",
    "else:\n",
    "    print(\"No i2b2 data found. Will use example data.\")\n",
    "    USE_I2B2 = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in i2b2 data and convert to spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_classes = \"EVENT\"\n",
    "labels = ['PROBLEM', 'TEST', 'TREATMENT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_i2b2_xmls(directory, nlp, num_docs=-1, annotation_classes=None, labels=None):\n",
    "    \"\"\"Read and parse all XML files in directory with suffix, which could be either\n",
    "    .xml for all classes, .extent for just Event classes, or tlink.\n",
    "    num_doc is an integer specifying how many docs to read in. Default is -1,\n",
    "    meaning all in the directory.\n",
    "\n",
    "    Returns a list of spaCy Docs.\n",
    "    \"\"\"\n",
    "    if annotation_classes is None:\n",
    "        annotation_classes = [\"EVENT\", \"TIMEX3\"]\n",
    "    docs = []\n",
    "    metas = []\n",
    "    assert os.path.exists(directory)\n",
    "    xml_files = glob.glob(os.path.join(directory, \"*.xml\"))\n",
    "    if num_docs == -1:\n",
    "        num_docs = len(xml_files)\n",
    "    for fname in xml_files[:num_docs]:\n",
    "        meta = {}\n",
    "        rpt_id = os.path.basename(fname).split('.')[0]\n",
    "        # First read in the xml document\n",
    "        \n",
    "        root = read_i2b2_xml(fname)\n",
    "        text = root.find('TEXT').text\n",
    "        doc = nlp(text)\n",
    "        meta[\"report_id\"] = rpt_id\n",
    "        meta[\"filepath\"] = fname\n",
    "\n",
    "        spans = []\n",
    "        span_tuples = set()\n",
    "        for anno_class in annotation_classes:\n",
    "            for tag in root.iter(anno_class):\n",
    "                try:\n",
    "                    span = ent_from_xml(tag, anno_class, doc)\n",
    "                    # If specific labels have been given, restrict to those annotations\n",
    "                    if labels is not None and span.label_ not in labels:\n",
    "                        continue\n",
    "                        \n",
    "                    span_tuple = (span.start, span.end)\n",
    "                    if span_tuple not in span_tuples:\n",
    "                        span_tuples.add(span_tuple)\n",
    "                        spans.append(span)\n",
    "                except Exception as e:\n",
    "                    raise e\n",
    "        doc.ents = tuple()\n",
    "        for span in spans:\n",
    "            try:\n",
    "                doc.ents += (span,)\n",
    "            except ValueError as e:\n",
    "                # TODO: merge overlapping annotations\n",
    "                print(rpt_id)\n",
    "                continue\n",
    "                raise e\n",
    "\n",
    "\n",
    "        docs.append(doc)\n",
    "    return docs\n",
    "\n",
    "def read_i2b2_xml(filepath):\n",
    "    try:\n",
    "        with open(filepath) as f:\n",
    "            xmlstring = f.read()\n",
    "        # NOTE - having '&' in the tags throws an error\n",
    "        # simple solution - replace them with '+' signs\n",
    "        xmlstring = re.sub('&', '+', xmlstring)\n",
    "        parser = ET.XMLParser(encoding='utf-8')\n",
    "        root = ET.fromstring(xmlstring, parser=parser)\n",
    "    except Exception as e:\n",
    "        print(\"Failed: {}\".format(xml))\n",
    "        print(e)\n",
    "        #raise e\n",
    "        return\n",
    "    return root\n",
    "\n",
    "def ent_from_xml(tag, label, doc):\n",
    "    \"\"\"Create a new Event Annotation object from an xml tag.\n",
    "    \"\"\"\n",
    "    ent_attrib = {}\n",
    "    anno_id = tag.attrib[\"id\"]\n",
    "    start = int(tag.attrib[\"start\"])\n",
    "    end = int(tag.attrib[\"end\"])\n",
    "    ent_attrib[\"type\"] = tag.attrib[\"type\"]\n",
    "    \n",
    "    document_span = doc.char_span(start, end)\n",
    "    \n",
    "    \n",
    "    ent = Span(doc, document_span.start, document_span.end, tag.attrib[\"type\"])\n",
    "    return ent\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_spacy_data(docs):\n",
    "    sents2ents = {}\n",
    "    for doc in docs:\n",
    "        for sent in doc.sents:\n",
    "            sents2ents.setdefault(sent, [])\n",
    "        for ent in doc.ents:\n",
    "            sent = ent.sent\n",
    "            sents2ents[sent].append(ent)\n",
    "    \n",
    "\n",
    "    data = []\n",
    "    for (sent, ents) in sents2ents.items():\n",
    "        # Tuple of start_char, end_char, label\n",
    "        annotations = []\n",
    "        for ent in ents:\n",
    "            if ent.label_ == '':\n",
    "                continue\n",
    "            annotation = (ent.start_char - sent.start_char, ent.end_char - sent.start_char, ent.label_)\n",
    "            if sent.text[annotation[0]:annotation[1]] == '':\n",
    "                continue\n",
    "            annotations.append(annotation)\n",
    "    \n",
    "        data.append((sent.text, {\"entities\": annotations}))\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517\n",
      "517\n",
      "517\n",
      "517\n",
      "517\n",
      "517\n",
      "676\n",
      "426\n",
      "156\n",
      "156\n",
      "168\n",
      "141\n",
      "151\n",
      "CPU times: user 23 s, sys: 1.96 s, total: 25 s\n",
      "Wall time: 28.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if USE_I2B2 is True:\n",
    "    training_docs = parse_i2b2_xmls(TRAINDIR, nlp, labels=labels, annotation_classes=[\"EVENT\"]) \n",
    "    testing_docs = parse_i2b2_xmls(TESTDIR, nlp, labels=labels, annotation_classes=[\"EVENT\"])\n",
    "    sents2ents = {}\n",
    "    for doc in training_docs:\n",
    "        for sent in doc.sents:\n",
    "            sents2ents.setdefault(sent, [])\n",
    "        for ent in doc.ents:\n",
    "            sent = ent.sent\n",
    "            sents2ents[sent].append(ent)\n",
    "    training_data = make_spacy_data(training_docs)\n",
    "    testing_data = make_spacy_data(testing_docs)\n",
    "else:\n",
    "    training_data = EXAMPLE_TRAIN_DATA\n",
    "    testing_data = EXAMPLE_TEST_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7640\n",
      "5821\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data))\n",
    "print(len(testing_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./my_saved_model\"\n",
    "n_iter = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(directory):\n",
    "    directory = Path(directory)\n",
    "    if not directory.exists():\n",
    "        directory.mkdir()\n",
    "    nlp.to_disk(directory)\n",
    "    print(\"Saved model to\", directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "- Iterate throug "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('PROBLEM', 'TEST', 'TREATMENT')\n",
      "0 / 250\n",
      "Losses {'ner': 31704.66421031952}\n",
      "1 / 250\n",
      "Losses {'ner': 25490.831581115723}\n",
      "2 / 250\n",
      "Losses {'ner': 22712.53517818451}\n",
      "3 / 250\n",
      "Losses {'ner': 21193.946857452393}\n",
      "4 / 250\n",
      "Losses {'ner': 19895.48380947113}\n",
      "5 / 250\n",
      "Losses {'ner': 18842.101117134094}\n",
      "6 / 250\n",
      "Losses {'ner': 17302.569165706635}\n",
      "7 / 250\n",
      "Losses {'ner': 16162.998267650604}\n",
      "8 / 250\n",
      "Losses {'ner': 15713.022518157959}\n",
      "9 / 250\n",
      "Losses {'ner': 14661.14384317398}\n",
      "10 / 250\n",
      "Losses {'ner': 13669.028096675873}\n",
      "11 / 250\n",
      "Losses {'ner': 12949.016832351685}\n",
      "12 / 250\n",
      "Losses {'ner': 12479.133256912231}\n",
      "13 / 250\n",
      "Losses {'ner': 11888.445013046265}\n",
      "14 / 250\n",
      "Losses {'ner': 11413.031613111496}\n",
      "15 / 250\n",
      "Losses {'ner': 11288.397390842438}\n",
      "16 / 250\n",
      "Losses {'ner': 10837.266374349594}\n",
      "17 / 250\n",
      "Losses {'ner': 10524.761800289154}\n",
      "18 / 250\n",
      "Losses {'ner': 10240.038081645966}\n",
      "19 / 250\n",
      "Losses {'ner': 9956.56172323227}\n",
      "20 / 250\n",
      "Losses {'ner': 9761.051298379898}\n",
      "21 / 250\n",
      "Losses {'ner': 9457.001722931862}\n",
      "22 / 250\n",
      "Losses {'ner': 9418.655989408493}\n",
      "23 / 250\n",
      "Losses {'ner': 9372.722046852112}\n",
      "24 / 250\n",
      "Losses {'ner': 9163.07813668251}\n",
      "25 / 250\n",
      "Losses {'ner': 9135.650406718254}\n",
      "26 / 250\n",
      "Losses {'ner': 8831.239758729935}\n",
      "27 / 250\n",
      "Losses {'ner': 8651.057097434998}\n",
      "28 / 250\n",
      "Losses {'ner': 8542.1416913867}\n",
      "29 / 250\n",
      "Losses {'ner': 8308.09082353115}\n",
      "30 / 250\n",
      "Losses {'ner': 8142.921384334564}\n",
      "31 / 250\n",
      "Losses {'ner': 8276.92614030838}\n",
      "32 / 250\n",
      "Losses {'ner': 7991.1493891477585}\n",
      "33 / 250\n",
      "Losses {'ner': 7836.440215349197}\n",
      "34 / 250\n",
      "Losses {'ner': 7900.400493144989}\n",
      "35 / 250\n",
      "Losses {'ner': 7672.355104923248}\n",
      "36 / 250\n",
      "Losses {'ner': 7744.765215992928}\n",
      "37 / 250\n",
      "Losses {'ner': 7575.13188791275}\n",
      "38 / 250\n",
      "Losses {'ner': 7598.592262506485}\n",
      "39 / 250\n",
      "Losses {'ner': 7368.203785859048}\n",
      "40 / 250\n",
      "Losses {'ner': 7267.540567994118}\n",
      "41 / 250\n",
      "Losses {'ner': 7229.9336260557175}\n",
      "42 / 250\n",
      "Losses {'ner': 7202.579539060593}\n",
      "43 / 250\n",
      "Losses {'ner': 7344.899646162987}\n",
      "44 / 250\n",
      "Losses {'ner': 6974.010159790516}\n",
      "45 / 250\n",
      "Losses {'ner': 6729.089902609587}\n",
      "46 / 250\n",
      "Losses {'ner': 6959.274307727814}\n",
      "47 / 250\n",
      "Losses {'ner': 6667.922772526741}\n",
      "48 / 250\n",
      "Losses {'ner': 6719.220711350441}\n",
      "49 / 250\n",
      "Losses {'ner': 6733.263006359339}\n",
      "50 / 250\n",
      "Losses {'ner': 6564.267406582832}\n",
      "51 / 250\n",
      "Losses {'ner': 6580.1230880618095}\n",
      "52 / 250\n",
      "Losses {'ner': 6618.538868844509}\n",
      "53 / 250\n",
      "Losses {'ner': 6422.216789662838}\n",
      "54 / 250\n",
      "Losses {'ner': 6433.862979531288}\n",
      "55 / 250\n",
      "Losses {'ner': 6431.857067883015}\n",
      "56 / 250\n",
      "Losses {'ner': 6402.49668776989}\n",
      "57 / 250\n",
      "Losses {'ner': 6322.082245647907}\n",
      "58 / 250\n",
      "Losses {'ner': 6296.526545763016}\n",
      "59 / 250\n",
      "Losses {'ner': 6175.271345973015}\n",
      "60 / 250\n",
      "Losses {'ner': 6068.0800386965275}\n",
      "61 / 250\n",
      "Losses {'ner': 6054.470018744469}\n",
      "62 / 250\n",
      "Losses {'ner': 6020.944614887238}\n",
      "63 / 250\n",
      "Losses {'ner': 5789.324306368828}\n",
      "64 / 250\n",
      "Losses {'ner': 5937.830835819244}\n",
      "65 / 250\n",
      "Losses {'ner': 5855.861518502235}\n",
      "66 / 250\n",
      "Losses {'ner': 5895.183515816927}\n",
      "67 / 250\n",
      "Losses {'ner': 5743.406933397055}\n",
      "68 / 250\n",
      "Losses {'ner': 5736.65203243494}\n",
      "69 / 250\n",
      "Losses {'ner': 5751.925678104162}\n",
      "70 / 250\n",
      "Losses {'ner': 5614.150194108486}\n",
      "71 / 250\n",
      "Losses {'ner': 5812.624520778656}\n",
      "72 / 250\n",
      "Losses {'ner': 5742.684566259384}\n",
      "73 / 250\n",
      "Losses {'ner': 5860.005176246166}\n",
      "74 / 250\n",
      "Losses {'ner': 5616.0545508265495}\n",
      "75 / 250\n",
      "Losses {'ner': 5426.016512840986}\n",
      "76 / 250\n",
      "Losses {'ner': 5644.347938477993}\n",
      "77 / 250\n",
      "Losses {'ner': 5497.405993998051}\n",
      "78 / 250\n",
      "Losses {'ner': 5421.07778057456}\n",
      "79 / 250\n",
      "Losses {'ner': 5508.521363258362}\n",
      "80 / 250\n",
      "Losses {'ner': 5332.670005917549}\n",
      "81 / 250\n",
      "Losses {'ner': 5479.004673361778}\n",
      "82 / 250\n",
      "Losses {'ner': 5316.570490241051}\n",
      "83 / 250\n",
      "Losses {'ner': 5377.614657521248}\n",
      "84 / 250\n",
      "Losses {'ner': 5220.252601861954}\n",
      "85 / 250\n",
      "Losses {'ner': 5201.589928269386}\n",
      "86 / 250\n",
      "Losses {'ner': 5136.601594001055}\n",
      "87 / 250\n",
      "Losses {'ner': 5034.335011899471}\n",
      "88 / 250\n",
      "Losses {'ner': 5104.915077418089}\n",
      "89 / 250\n",
      "Losses {'ner': 5188.429025948048}\n",
      "90 / 250\n",
      "Losses {'ner': 5202.531730741262}\n",
      "91 / 250\n",
      "Losses {'ner': 5184.305504441261}\n",
      "92 / 250\n",
      "Losses {'ner': 5050.322225481272}\n",
      "93 / 250\n",
      "Losses {'ner': 4998.183579206467}\n",
      "94 / 250\n",
      "Losses {'ner': 5046.251204907894}\n",
      "95 / 250\n",
      "Losses {'ner': 5134.5395445376635}\n",
      "96 / 250\n",
      "Losses {'ner': 4983.136063888669}\n",
      "97 / 250\n",
      "Losses {'ner': 4829.263530552387}\n",
      "98 / 250\n",
      "Losses {'ner': 4974.5250161886215}\n",
      "99 / 250\n",
      "Losses {'ner': 5026.922051995993}\n",
      "100 / 250\n",
      "Losses {'ner': 4911.61363619566}\n",
      "101 / 250\n",
      "Losses {'ner': 4888.321142613888}\n",
      "102 / 250\n",
      "Losses {'ner': 4846.232992112637}\n",
      "103 / 250\n",
      "Losses {'ner': 4720.695565059781}\n",
      "104 / 250\n",
      "Losses {'ner': 4687.086690843105}\n",
      "105 / 250\n",
      "Losses {'ner': 4693.163910597563}\n",
      "106 / 250\n",
      "Losses {'ner': 4762.918801367283}\n",
      "107 / 250\n",
      "Losses {'ner': 4620.2822292894125}\n",
      "108 / 250\n",
      "Losses {'ner': 4546.643167823553}\n",
      "109 / 250\n",
      "Losses {'ner': 4782.002153158188}\n",
      "110 / 250\n",
      "Losses {'ner': 4735.962230414152}\n",
      "111 / 250\n",
      "Losses {'ner': 4657.80167555809}\n",
      "112 / 250\n",
      "Losses {'ner': 4622.232635192573}\n",
      "113 / 250\n",
      "Losses {'ner': 4549.629335641861}\n",
      "114 / 250\n",
      "Losses {'ner': 4492.386394739151}\n",
      "115 / 250\n",
      "Losses {'ner': 4601.838796302676}\n",
      "116 / 250\n",
      "Losses {'ner': 4562.639156609774}\n",
      "117 / 250\n",
      "Losses {'ner': 4419.065567493439}\n",
      "118 / 250\n",
      "Losses {'ner': 4524.6802388727665}\n",
      "119 / 250\n",
      "Losses {'ner': 4522.197468727827}\n",
      "120 / 250\n",
      "Losses {'ner': 4493.4907573238015}\n",
      "121 / 250\n",
      "Losses {'ner': 4543.015169173479}\n",
      "122 / 250\n",
      "Losses {'ner': 4497.115829333663}\n",
      "123 / 250\n",
      "Losses {'ner': 4285.100033894181}\n",
      "124 / 250\n",
      "Losses {'ner': 4465.9004103168845}\n",
      "125 / 250\n",
      "Losses {'ner': 4516.87550342083}\n",
      "126 / 250\n",
      "Losses {'ner': 4437.142564371228}\n",
      "127 / 250\n",
      "Losses {'ner': 4268.232624769211}\n",
      "128 / 250\n",
      "Losses {'ner': 4258.326913654804}\n",
      "129 / 250\n",
      "Losses {'ner': 4337.509189218283}\n",
      "130 / 250\n",
      "Losses {'ner': 4266.57621794939}\n",
      "131 / 250\n",
      "Losses {'ner': 4313.381675705314}\n",
      "132 / 250\n",
      "Losses {'ner': 4452.4214732944965}\n",
      "133 / 250\n",
      "Losses {'ner': 4234.2082395628095}\n",
      "134 / 250\n",
      "Losses {'ner': 4210.274978030473}\n",
      "135 / 250\n",
      "Losses {'ner': 4208.864009050652}\n",
      "136 / 250\n",
      "Losses {'ner': 4112.213305443525}\n",
      "137 / 250\n",
      "Losses {'ner': 4159.882860377431}\n",
      "138 / 250\n",
      "Losses {'ner': 4038.429098844528}\n",
      "139 / 250\n",
      "Losses {'ner': 4083.155158691108}\n",
      "140 / 250\n",
      "Losses {'ner': 4154.1379343904555}\n",
      "141 / 250\n",
      "Losses {'ner': 4232.904263079166}\n",
      "142 / 250\n",
      "Losses {'ner': 4114.841516964138}\n",
      "143 / 250\n",
      "Losses {'ner': 4085.343557536602}\n",
      "144 / 250\n",
      "Losses {'ner': 4003.3094898611307}\n",
      "145 / 250\n",
      "Losses {'ner': 4231.827976420522}\n",
      "146 / 250\n",
      "Losses {'ner': 4167.168512940407}\n",
      "147 / 250\n",
      "Losses {'ner': 3974.708021096885}\n",
      "148 / 250\n",
      "Losses {'ner': 4148.0013301074505}\n",
      "149 / 250\n",
      "Losses {'ner': 3943.147226765752}\n",
      "150 / 250\n",
      "Losses {'ner': 3998.185888417065}\n",
      "151 / 250\n",
      "Losses {'ner': 4133.884517878294}\n",
      "152 / 250\n",
      "Losses {'ner': 4053.1036014184356}\n",
      "153 / 250\n",
      "Losses {'ner': 4043.3821833729744}\n",
      "154 / 250\n",
      "Losses {'ner': 4054.4686164557934}\n",
      "155 / 250\n",
      "Losses {'ner': 4010.5548877716064}\n",
      "156 / 250\n",
      "Losses {'ner': 3920.0342913269997}\n",
      "157 / 250\n",
      "Losses {'ner': 3908.574618436396}\n",
      "158 / 250\n",
      "Losses {'ner': 4002.7086019665003}\n",
      "159 / 250\n",
      "Losses {'ner': 3904.540934637189}\n",
      "160 / 250\n",
      "Losses {'ner': 3950.9257619976997}\n",
      "161 / 250\n",
      "Losses {'ner': 3691.7154171019793}\n",
      "162 / 250\n",
      "Losses {'ner': 3893.809599302709}\n",
      "163 / 250\n",
      "Losses {'ner': 3902.4993315786123}\n",
      "164 / 250\n",
      "Losses {'ner': 3779.008343562484}\n",
      "165 / 250\n",
      "Losses {'ner': 3857.2698105722666}\n",
      "166 / 250\n",
      "Losses {'ner': 3857.3860678970814}\n",
      "167 / 250\n",
      "Losses {'ner': 3724.0570659190416}\n",
      "168 / 250\n",
      "Losses {'ner': 3737.74743049033}\n",
      "169 / 250\n",
      "Losses {'ner': 3927.229135297239}\n",
      "170 / 250\n",
      "Losses {'ner': 3822.8395422250032}\n",
      "171 / 250\n",
      "Losses {'ner': 3605.5497481673956}\n",
      "172 / 250\n",
      "Losses {'ner': 3746.9102756232023}\n",
      "173 / 250\n",
      "Losses {'ner': 3930.8702416419983}\n",
      "174 / 250\n",
      "Losses {'ner': 3803.7023865282536}\n",
      "175 / 250\n",
      "Losses {'ner': 3761.1945091784}\n",
      "176 / 250\n",
      "Losses {'ner': 3687.680519193411}\n",
      "177 / 250\n",
      "Losses {'ner': 3602.9944780617952}\n",
      "178 / 250\n",
      "Losses {'ner': 3635.73855156824}\n",
      "179 / 250\n",
      "Losses {'ner': 3724.0077735707164}\n",
      "180 / 250\n",
      "Losses {'ner': 3632.3766485229135}\n",
      "181 / 250\n",
      "Losses {'ner': 3707.0430290848017}\n",
      "182 / 250\n",
      "Losses {'ner': 3699.5257874429226}\n",
      "183 / 250\n",
      "Losses {'ner': 3704.9478075504303}\n",
      "184 / 250\n",
      "Losses {'ner': 3744.5227683372796}\n",
      "185 / 250\n",
      "Losses {'ner': 3744.7533553913236}\n",
      "186 / 250\n",
      "Losses {'ner': 3709.276580095291}\n",
      "187 / 250\n",
      "Losses {'ner': 3625.237950488925}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188 / 250\n",
      "Losses {'ner': 3539.919009283185}\n",
      "189 / 250\n",
      "Losses {'ner': 3555.1445163879544}\n",
      "190 / 250\n",
      "Losses {'ner': 3581.314238026738}\n",
      "191 / 250\n",
      "Losses {'ner': 3621.02473615855}\n",
      "192 / 250\n",
      "Losses {'ner': 3451.8410449400544}\n",
      "193 / 250\n",
      "Losses {'ner': 3571.260935753584}\n",
      "194 / 250\n",
      "Losses {'ner': 3652.4605939537287}\n",
      "195 / 250\n",
      "Losses {'ner': 3622.4723713845015}\n",
      "196 / 250\n",
      "Losses {'ner': 3482.4711535573006}\n",
      "197 / 250\n",
      "Losses {'ner': 3570.8038476035}\n",
      "198 / 250\n",
      "Losses {'ner': 3547.090422645211}\n",
      "199 / 250\n",
      "Losses {'ner': 3542.04293128103}\n",
      "200 / 250\n",
      "Losses {'ner': 3538.650417484343}\n",
      "201 / 250\n",
      "Losses {'ner': 3512.7869878299534}\n",
      "202 / 250\n",
      "Losses {'ner': 3490.9966707602143}\n",
      "203 / 250\n",
      "Losses {'ner': 3517.351877063513}\n",
      "204 / 250\n",
      "Losses {'ner': 3394.517757333815}\n",
      "205 / 250\n",
      "Losses {'ner': 3488.210429377854}\n",
      "206 / 250\n",
      "Losses {'ner': 3473.238218039274}\n",
      "207 / 250\n",
      "Losses {'ner': 3484.7658082842827}\n",
      "208 / 250\n",
      "Losses {'ner': 3368.0741888284683}\n",
      "209 / 250\n",
      "Losses {'ner': 3240.6222820729017}\n",
      "210 / 250\n",
      "Losses {'ner': 3368.2240307852626}\n",
      "211 / 250\n",
      "Losses {'ner': 3370.0225921943784}\n",
      "212 / 250\n",
      "Losses {'ner': 3339.2545001134276}\n",
      "213 / 250\n",
      "Losses {'ner': 3367.062520727515}\n",
      "214 / 250\n",
      "Losses {'ner': 3385.2683244794607}\n",
      "215 / 250\n",
      "Losses {'ner': 3420.0981503799558}\n",
      "216 / 250\n",
      "Losses {'ner': 3476.0494044572115}\n",
      "217 / 250\n",
      "Losses {'ner': 3483.287118703127}\n",
      "218 / 250\n",
      "Losses {'ner': 3469.788171634078}\n",
      "219 / 250\n",
      "Losses {'ner': 3352.9928838908672}\n",
      "220 / 250\n",
      "Losses {'ner': 3299.4209389984608}\n",
      "221 / 250\n",
      "Losses {'ner': 3440.6890131160617}\n",
      "222 / 250\n",
      "Losses {'ner': 3206.1877278760076}\n",
      "223 / 250\n",
      "Losses {'ner': 3342.15050894022}\n",
      "224 / 250\n",
      "Losses {'ner': 3289.959895297885}\n",
      "225 / 250\n",
      "Losses {'ner': 3359.7474924251437}\n",
      "226 / 250\n",
      "Losses {'ner': 3341.3975280746818}\n",
      "227 / 250\n",
      "Losses {'ner': 3281.862893078476}\n",
      "228 / 250\n",
      "Losses {'ner': 3355.9900984689593}\n",
      "229 / 250\n",
      "Losses {'ner': 3318.8054317757487}\n",
      "230 / 250\n",
      "Losses {'ner': 3250.488007083535}\n",
      "231 / 250\n",
      "Losses {'ner': 3372.6583023518324}\n",
      "232 / 250\n",
      "Losses {'ner': 3398.7933702021837}\n",
      "233 / 250\n",
      "Losses {'ner': 3281.2356765791774}\n",
      "234 / 250\n",
      "Losses {'ner': 3265.7785077244043}\n",
      "235 / 250\n",
      "Losses {'ner': 3310.220546603203}\n",
      "236 / 250\n",
      "Losses {'ner': 3334.947635576129}\n",
      "237 / 250\n",
      "Losses {'ner': 3233.724803522229}\n",
      "238 / 250\n",
      "Losses {'ner': 3101.616370305419}\n",
      "239 / 250\n",
      "Losses {'ner': 3225.119266450405}\n",
      "240 / 250\n",
      "Losses {'ner': 3151.024725854397}\n",
      "241 / 250\n",
      "Losses {'ner': 3258.8069270439446}\n",
      "242 / 250\n",
      "Losses {'ner': 3271.6928791590035}\n",
      "243 / 250\n",
      "Losses {'ner': 3236.3081970214844}\n",
      "244 / 250\n",
      "Losses {'ner': 3216.3133859187365}\n",
      "245 / 250\n",
      "Losses {'ner': 3151.4712790995836}\n",
      "246 / 250\n",
      "Losses {'ner': 3305.8336904756725}\n",
      "247 / 250\n",
      "Losses {'ner': 3208.047176104039}\n",
      "248 / 250\n",
      "Losses {'ner': 3157.2139185369015}\n",
      "249 / 250\n",
      "Losses {'ner': 3161.331735379994}\n",
      "CPU times: user 1h 45s, sys: 7min 17s, total: 1h 8min 2s\n",
      "Wall time: 2h 45min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if \"ner\" in nlp.pipe_names:\n",
    "    nlp.remove_pipe(\"ner\")\n",
    "ner = nlp.create_pipe(\"ner\")\n",
    "nlp.add_pipe(ner, last=True)\n",
    "\n",
    "# add labels\n",
    "for _, annotations in training_data:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        label = ent[2]\n",
    "        if label in labels: # Limit to the subset of trained labels which we're interested in\n",
    "            ner.add_label(ent[2])\n",
    "print(ner.labels)\n",
    "\n",
    "\n",
    "        \n",
    "# get names of other pipes to disable them during training\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "    # reset and initialize the weights randomly – but only if we're\n",
    "    # training a new model\n",
    "\n",
    "    nlp.begin_training()\n",
    "    for itn in range(n_iter):\n",
    "        random.shuffle(training_data)\n",
    "        losses = {}\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        batches = minibatch(training_data, size=compounding(50.0, 32.0, 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(\n",
    "                texts,  # batch of texts\n",
    "                annotations,  # batch of annotations\n",
    "                drop=0.5,  # dropout - make it harder to memorise data\n",
    "                losses=losses,\n",
    "            )\n",
    "        if itn % 1 == 0:\n",
    "            print(f\"{itn} / {n_iter}\")\n",
    "            print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to my_saved_model\n"
     ]
    }
   ],
   "source": [
    "save_model(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and evaluate model\n",
    "Now, you can load your trained model and use it just like you would with any other model! Let's load and test it on our testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_nlp = spacy.load(\"my_saved_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 74.36894210335355\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test data\n",
    "scorer = new_nlp.evaluate(testing_data)\n",
    "print(\"F1:\", scorer.ents_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: []\n",
      "Expected: []\n",
      "Tokens [('\\n', '', 2), ('Admission', '', 2), ('Date', '', 2), (':', '', 2), ('\\n', '', 2), ('2012', '', 2), ('-', '', 2), ('06', '', 2), ('-', '', 2), ('07', '', 2), ('\\n', '', 2)]\n",
      "Predicted: []\n",
      "Expected: []\n",
      "Tokens [('Discharge', '', 2), ('Date', '', 2), (':', '', 2), ('\\n', '', 2)]\n",
      "Predicted: []\n",
      "Expected: []\n",
      "Tokens [('2012', '', 2), ('-', '', 2), ('06', '', 2), ('-', '', 2), ('09', '', 2), ('\\n', '', 2), ('Service', '', 2), (':', '', 2), ('\\n', '', 2)]\n",
      "Predicted: []\n",
      "Expected: []\n",
      "Tokens [('MEDICINE', '', 2), ('\\n', '', 2), ('History', '', 2), ('of', '', 2), ('Present', '', 2), ('Illness', '', 2), (':', '', 2), ('\\n', '', 2)]\n",
      "Predicted: [(51, 62, 'PROBLEM'), (65, 81, 'PROBLEM'), (233, 238, 'TREATMENT')]\n",
      "Expected: [(51, 62, 'PROBLEM'), (65, 81, 'PROBLEM'), (87, 103, 'PROBLEM'), (233, 238, 'TREATMENT')]\n",
      "Tokens [('Mr.', '', 2), ('Vazquez', '', 2), ('is', '', 2), ('a', '', 2), ('48', '', 2), ('year', '', 2), ('old', '', 2), ('man', '', 2), ('with', '', 2), ('a', '', 2), ('history', '', 2), ('of', '', 2), ('hepatitis', 'PROBLEM', 3), ('C', 'PROBLEM', 1), (',', '', 2), ('bipolar', 'PROBLEM', 3), ('disorder', 'PROBLEM', 1), ('with', '', 2), ('suicide', '', 2), ('attempts', '', 2), ('in', '', 2), ('the', '', 2), ('past', '', 2), (',', '', 2), ('recent', '', 2), ('CMED', '', 2), ('admit', '', 2), ('earlier', '', 2), ('in', '', 2), ('4/05', '', 2), ('who', '', 2), ('was', '', 2), ('found', '', 2), ('alert', '', 2), ('approximately', '', 2), ('one', '', 2), ('hour', '', 2), ('after', '', 2), ('taking', '', 2), ('an', '', 2), ('unspecified', '', 2), ('number', '', 2), ('of', '', 2), ('pills', 'TREATMENT', 3), ('.', '', 2), ('\\n', '', 2)]\n",
      "Predicted: [(24, 48, 'TREATMENT'), (53, 61, 'TREATMENT'), (123, 133, 'TREATMENT'), (136, 142, 'TREATMENT'), (147, 157, 'PROBLEM')]\n",
      "Expected: [(41, 48, 'TREATMENT'), (53, 61, 'TREATMENT'), (114, 120, 'TREATMENT'), (123, 133, 'TREATMENT'), (136, 142, 'TREATMENT'), (147, 157, 'TREATMENT')]\n",
      "Tokens [('Per', '', 2), ('report', '', 2), (',', '', 2), ('there', '', 2), ('were', '', 2), ('empty', 'TREATMENT', 3), ('bottles', 'TREATMENT', 1), ('of', 'TREATMENT', 1), ('Inderal', 'TREATMENT', 1), ('and', '', 2), ('Klonopin', 'TREATMENT', 3), ('found', '', 2), ('at', '', 2), ('bedside', '', 2), (',', '', 2), ('as', '', 2), ('well', '', 2), ('as', '', 2), ('02', '', 2), ('-', '', 2), ('26', '', 2), ('full', '', 2), ('bottles', '', 2), ('of', '', 2), ('geodon', '', 2), (',', '', 2), ('gabapentin', 'TREATMENT', 3), (',', '', 2), ('Lescol', 'TREATMENT', 3), ('and', '', 2), ('paroxitine', 'PROBLEM', 3), ('.', '', 2), ('\\n', '', 2)]\n",
      "Predicted: [(60, 72, 'PROBLEM')]\n",
      "Expected: [(60, 72, 'PROBLEM')]\n",
      "Tokens [('The', '', 2), ('patient', '', 2), ('was', '', 2), ('reportedly', '', 2), ('alert', '', 2), ('at', '', 2), ('scene', '', 2), (',', '', 2), ('and', '', 2), ('then', '', 2), ('became', '', 2), ('unresponsive', 'PROBLEM', 3), ('in', '', 2), ('route', '', 2), ('to', '', 2), ('the', '', 2), ('hospital', '', 2), ('.', '', 2), ('\\n', '', 2)]\n",
      "Predicted: [(30, 39, 'TREATMENT'), (50, 68, 'TREATMENT')]\n",
      "Expected: [(30, 39, 'TREATMENT'), (50, 68, 'TREATMENT')]\n",
      "Tokens [('In', '', 2), ('the', '', 2), ('Rita', '', 2), (',', '', 2), ('the', '', 2), ('patient', '', 2), ('was', '', 2), ('intubated', 'TREATMENT', 3), ('and', '', 2), ('given', '', 2), ('activated', 'TREATMENT', 3), ('charcoal', 'TREATMENT', 1), ('.', '', 2), ('\\n', '', 2)]\n",
      "Predicted: [(0, 14, 'TEST'), (29, 36, 'TEST')]\n",
      "Expected: [(0, 14, 'TEST'), (29, 36, 'TEST')]\n",
      "Tokens [('His', 'TEST', 3), ('initial', 'TEST', 1), ('BP', 'TEST', 1), ('was', '', 2), ('60', '', 2), ('/', '', 2), ('P', '', 2), ('with', '', 2), ('a', 'TEST', 3), ('pulse', 'TEST', 1), ('in', '', 2), ('the', '', 2), ('60', '', 2), ('s.', '', 2), ('\\n', '', 2)]\n",
      "Predicted: [(23, 40, 'TREATMENT'), (58, 71, 'TREATMENT')]\n",
      "Expected: [(23, 40, 'TREATMENT'), (58, 71, 'TREATMENT')]\n",
      "Tokens [('He', '', 2), ('was', '', 2), ('given', '', 2), ('4', '', 2), ('amps', '', 2), ('of', '', 2), ('calcium', 'TREATMENT', 3), ('gluconate', 'TREATMENT', 1), (',', '', 2), ('and', '', 2), ('started', '', 2), ('on', '', 2), ('a', 'TREATMENT', 3), ('Calcium', 'TREATMENT', 1), ('gtt', 'TREATMENT', 1), ('.', '', 2), ('\\n', '', 2)]\n"
     ]
    }
   ],
   "source": [
    "# Let's print out some of our predictions\n",
    "for text, data in testing_data[:10]:\n",
    "    doc = new_nlp(text)\n",
    "    predicted = [(ent.start_char, ent.end_char, ent.label_)\n",
    "                         for ent in doc.ents]\n",
    "    print(\"Predicted:\", predicted)\n",
    "    print(\"Expected:\", data[\"entities\"])\n",
    "    print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
